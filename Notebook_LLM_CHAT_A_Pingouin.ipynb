{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e430db1e-6e7e-468b-bff5-86dfe825dd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda | DTYPE: torch.float16\n",
      "BASE_MODEL: meta-llama/Llama-3.2-1B-Instruct\n",
      "EVAL_MODEL: Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1 â€” IMPORT & CONFIG\n",
    "# ============================================================\n",
    "import os, json, math, random, re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "DATA_JSON_PATH = \"dataset_qa.json\"\n",
    "SPLIT_DIR      = \"splits_json\"\n",
    "OUTPUT_DIR     = \"output_lora\"\n",
    "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Models\n",
    "# -------------------------\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"   # gated (but you already got access)\n",
    "EVAL_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"         # evaluator model (scoring 0â€“5)\n",
    "\n",
    "# -------------------------\n",
    "# Training hyperparams\n",
    "# -------------------------\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 2\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "EPOCHS = 3\n",
    "LR = 2e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.06\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Generation (prediksi test set)\n",
    "GEN_MAX_NEW_TOKENS = 256\n",
    "GEN_TEMPERATURE = 0.7\n",
    "GEN_TOP_P = 0.95\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE  = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(\"DEVICE:\", DEVICE, \"| DTYPE:\", DTYPE)\n",
    "print(\"BASE_MODEL:\", BASE_MODEL)\n",
    "print(\"EVAL_MODEL:\", EVAL_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd322ad-76c4-4978-bc0f-d659b6aed409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token :  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] HF login done\n"
     ]
    }
   ],
   "source": [
    "## ============================================================\n",
    "# CELL 2 â€” HF LOGIN TOKEN | \n",
    "# ============================================================\n",
    "# Cara paling aman: paste token sekali per runtime\n",
    "# Token ambil dari HF: Settings -> Access Tokens (Read)\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None) \n",
    "if HF_TOKEN is None:\n",
    "    HF_TOKEN = getpass(\"Token : \")\n",
    "\n",
    "login(HF_TOKEN)\n",
    "print(\"[OK] HF login done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38279b15-de38-4550-89c5-6c97a8709ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Total records: 170\n",
      "Contoh 1 record:\n",
      " {\n",
      "  \"instruction\": \"Jawab pertanyaan pengguna berikut dengan jelas, sopan, dan akurat terkait informasi Mirota Kampus.\",\n",
      "  \"input\": \"Apa itu Mirota Kampus?\",\n",
      "  \"output\": \"Mirota Kampus adalah jaringan toko retail di Yogyakarta yang menjual berbagai kebutuhan mahasiswa dan masyarakat, termasuk alat tulis, kosmetik, sembako, snack, dan perlengkapan rumah tangga.\"\n",
      "}\n",
      "train: 136 val: 17 test: 17\n",
      "[OK] Split saved in: splits_json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3 â€” LOAD JSON + VALIDASI + SPLIT (train/val/test)\n",
    "# Rubrik: Dataset + Data Preparation\n",
    "# ============================================================\n",
    "if not os.path.exists(DATA_JSON_PATH):\n",
    "    raise FileNotFoundError(f\"File JSON tidak ditemukan: {DATA_JSON_PATH}\")\n",
    "\n",
    "with open(DATA_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# aman jika format {\"data\":[...]}\n",
    "if isinstance(data, dict) and \"data\" in data and isinstance(data[\"data\"], list):\n",
    "    data = data[\"data\"]\n",
    "\n",
    "def validate_records(records: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "    if not isinstance(records, list):\n",
    "        raise ValueError(\"JSON harus list of dict\")\n",
    "\n",
    "    cleaned: List[Dict[str, str]] = []\n",
    "    for i, r in enumerate(records):\n",
    "        if not isinstance(r, dict):\n",
    "            raise ValueError(f\"Record #{i} bukan dict: {type(r)}\")\n",
    "\n",
    "        for k in (\"instruction\", \"input\", \"output\"):\n",
    "            if k not in r:\n",
    "                raise ValueError(f\"Record #{i} tidak punya key '{k}'. Keys: {list(r.keys())}\")\n",
    "\n",
    "        instr = str(r[\"instruction\"]).strip()\n",
    "        inp   = str(r[\"input\"]).strip()\n",
    "        out   = str(r[\"output\"]).strip()\n",
    "\n",
    "        if instr == \"\" or out == \"\":\n",
    "            raise ValueError(f\"Record #{i} instruction/output kosong\")\n",
    "\n",
    "        cleaned.append({\"instruction\": instr, \"input\": inp, \"output\": out})\n",
    "    return cleaned\n",
    "\n",
    "data = validate_records(data)\n",
    "print(\"[OK] Total records:\", len(data))\n",
    "print(\"Contoh 1 record:\\n\", json.dumps(data[0], ensure_ascii=False, indent=2))\n",
    "\n",
    "train_data, temp_data = train_test_split(\n",
    "    data, test_size=0.2, random_state=SEED, shuffle=True\n",
    ")\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data, test_size=0.5, random_state=SEED, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"train:\", len(train_data), \"val:\", len(val_data), \"test:\", len(test_data))\n",
    "\n",
    "with open(os.path.join(SPLIT_DIR, \"train.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "with open(os.path.join(SPLIT_DIR, \"val.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "with open(os.path.join(SPLIT_DIR, \"test.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[OK] Split saved in:\", SPLIT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7cc1b35-eb3e-4a60-898f-4475fd5088bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh user_text:\n",
      " Jawab pertanyaan pengguna berikut dengan jelas, sopan, dan akurat terkait informasi Mirota Kampus.\n",
      "\n",
      "Input:\n",
      "Kalau cari ATK buat kuliah lengkap nggak di sana?\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4 â€” TOKENIZER + PROMPT FORMATTING (chat template)\n",
    "# Rubrik: Tokenization & Prompt Formatting\n",
    "# ============================================================\n",
    "def tok_from_pretrained(name: str, token: str):\n",
    "    try:\n",
    "        return AutoTokenizer.from_pretrained(name, use_fast=True, token=token)\n",
    "    except TypeError:\n",
    "        return AutoTokenizer.from_pretrained(name, use_fast=True, use_auth_token=token)\n",
    "\n",
    "tokenizer = tok_from_pretrained(BASE_MODEL, HF_TOKEN)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "SYSTEM_PROMPT = \"Kamu adalah asisten AI yang membantu dan menjawab dengan jelas.\"\n",
    "\n",
    "def build_user_text(instruction: str, user_input: str) -> str:\n",
    "    instruction = instruction.strip()\n",
    "    user_input = (user_input or \"\").strip()\n",
    "    if user_input:\n",
    "        return f\"{instruction}\\n\\nInput:\\n{user_input}\"\n",
    "    return instruction\n",
    "\n",
    "def encode_prompt_and_full(instruction: str, user_input: str, answer: str, max_length: int):\n",
    "    \"\"\"\n",
    "    prompt_ids: system+user sampai titik assistant (generation prompt)\n",
    "    full_ids  : system+user+assistant(answer)\n",
    "    labels    : -100 untuk prompt, token jawaban dihitung loss\n",
    "    \"\"\"\n",
    "    user_text = build_user_text(instruction, user_input)\n",
    "    answer = answer.strip()\n",
    "\n",
    "    messages_prompt = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    prompt_ids = tokenizer.apply_chat_template(\n",
    "        messages_prompt,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    messages_full = messages_prompt + [{\"role\": \"assistant\", \"content\": answer}]\n",
    "    full_ids = tokenizer.apply_chat_template(\n",
    "        messages_full,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    prompt_len = len(prompt_ids)\n",
    "    labels = [-100] * min(prompt_len, len(full_ids)) + full_ids[min(prompt_len, len(full_ids)):]\n",
    "    labels = labels[:len(full_ids)]\n",
    "    return full_ids, labels\n",
    "\n",
    "print(\"Contoh user_text:\\n\", build_user_text(train_data[0][\"instruction\"], train_data[0][\"input\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b69aa1b7-0faf-43dd-94d1-84183ae58065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:00<00:00, 605.68it/s]\n",
      "Encoding dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 427.13it/s]\n",
      "Encoding dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 341.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes: train=136 val=17 test=17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5 â€” DATASET + COLLATE FUNCTION\n",
    "# Rubrik: InstructionDataset + custom_collate_fn\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class EncodedExample:\n",
    "    input_ids: List[int]\n",
    "    labels: List[int]\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, records: List[Dict[str, str]], max_length: int = 512):\n",
    "        self.records = records\n",
    "        self.max_length = max_length\n",
    "        self.encoded: List[EncodedExample] = []\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        for r in tqdm(self.records, desc=\"Encoding dataset\"):\n",
    "            full_ids, labels = encode_prompt_and_full(\n",
    "                r[\"instruction\"], r[\"input\"], r[\"output\"], self.max_length\n",
    "            )\n",
    "            self.encoded.append(EncodedExample(input_ids=full_ids, labels=labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.encoded[idx]\n",
    "        return {\"input_ids\": ex.input_ids, \"labels\": ex.labels}\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = [torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch]\n",
    "    labels    = [torch.tensor(x[\"labels\"], dtype=torch.long) for x in batch]\n",
    "\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels, batch_first=True, padding_value=-100\n",
    "    )\n",
    "\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "train_ds = InstructionDataset(train_data, max_length=MAX_LENGTH)\n",
    "val_ds   = InstructionDataset(val_data,   max_length=MAX_LENGTH)\n",
    "test_ds  = InstructionDataset(test_data,  max_length=MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=custom_collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=1,         shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "print(f\"sizes: train={len(train_ds)} val={len(val_ds)} test={len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce631fae-485c-4f86-8fb5-1d4e5bb1c134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6 â€” LOAD BASE MODEL + LoRA\n",
    "# Rubrik: Fine-Tuning LLM (load model, pindah device)\n",
    "# ============================================================\n",
    "def model_from_pretrained(name: str, token: str, dtype):\n",
    "    try:\n",
    "        return AutoModelForCausalLM.from_pretrained(name, token=token, torch_dtype=dtype)\n",
    "    except TypeError:\n",
    "        return AutoModelForCausalLM.from_pretrained(name, use_auth_token=token, torch_dtype=dtype)\n",
    "\n",
    "base_model = model_from_pretrained(BASE_MODEL, HF_TOKEN, DTYPE)\n",
    "base_model.to(DEVICE)\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71d42df6-63ca-43d6-a7d5-ea42a09be88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1427855/3310612489.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "/tmp/ipykernel_1427855/3310612489.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline train_loss: 2.4883751413401436\n",
      "Baseline val_loss  : 2.5526473124821982\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7 â€” BASELINE TRAIN/VAL LOSS (SEBELUM TRAINING)\n",
    "# Rubrik: hitung train/val loss awal\n",
    "# ============================================================\n",
    "use_amp = (DEVICE == \"cuda\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss(dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            out = model(**batch)\n",
    "            losses.append(out.loss.item())\n",
    "    model.train()\n",
    "    return float(np.mean(losses)) if losses else 0.0\n",
    "\n",
    "train_loss0 = evaluate_loss(train_loader)\n",
    "val_loss0   = evaluate_loss(val_loader)\n",
    "print(\"Baseline train_loss:\", train_loss0)\n",
    "print(\"Baseline val_loss  :\", val_loss0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98b906ac-2a25-4bc0-85b1-79e3e93926fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1427855/3911211188.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:13<00:00,  4.97it/s, loss=0.775]\n",
      "/tmp/ipykernel_1427855/3310612489.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] val_loss=1.4080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:13<00:00,  5.12it/s, loss=0.607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] val_loss=1.2403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:13<00:00,  4.92it/s, loss=0.539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] val_loss=1.1914\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 8 â€” TRAIN LOOP (train_model_simple)\n",
    "# Rubrik: train_model_simple + training loop\n",
    "# ============================================================\n",
    "def train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs: int = 3,\n",
    "    lr: float = 2e-4,\n",
    "    grad_accum_steps: int = 8,\n",
    "    weight_decay: float = 0.01,\n",
    "    warmup_ratio: float = 0.06,\n",
    "    max_grad_norm: float = 1.0,\n",
    "):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    total_steps  = math.ceil(len(train_loader) / grad_accum_steps) * epochs\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        running = 0.0\n",
    "\n",
    "        for step, batch in enumerate(pbar, start=1):\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                out = model(**batch)\n",
    "                loss = out.loss / grad_accum_steps\n",
    "\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            running += loss.item()\n",
    "\n",
    "            do_step = (step % grad_accum_steps == 0)\n",
    "            is_last = (step == len(train_loader))\n",
    "            if do_step or is_last:\n",
    "                if use_amp:\n",
    "                    scaler.unscale_(optimizer)\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                if use_amp:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scheduler.step()\n",
    "\n",
    "                pbar.set_postfix({\"loss\": float(running)})\n",
    "                running = 0.0\n",
    "\n",
    "        val_loss = evaluate_loss(val_loader)\n",
    "        print(f\"[Epoch {epoch}] val_loss={val_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    grad_accum_steps=GRAD_ACCUM_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b265df5-ca7c-47bf-8039-5e3551364524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved adapter to: output_lora\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 9 â€” SAVE ADAPTER + TOKENIZER\n",
    "# Rubrik: deployment preparation (artifacts)\n",
    "# ============================================================\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"base_model\": BASE_MODEL}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[OK] saved adapter to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c958ce90-41c8-46fe-a47a-cda060f6935c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:18<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved: predictions_test.json\n",
      "Sample prediction:\n",
      " {\n",
      "  \"instruction\": \"Jawab pertanyaan pengguna berikut dengan jelas, sopan, dan akurat terkait informasi Mirota Kampus.\",\n",
      "  \"input\": \"Mirota bisa jadi tempat belanja anak kos nggak?\",\n",
      "  \"output\": \"Bisa, karena Mirota Kampus menyediakan banyak kebutuhan anak kos dan mahasiswa.\",\n",
      "  \"model_response\": \"Ya, Mirota Kampus menyediakan perlengkapan kos anak, perlengkapan rumah tangga, dan kebutuhan hiburan.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 10 â€” GENERATE PREDICTIONS TEST SET + SAVE JSON\n",
    "# Rubrik: Model Prediction / Demo\n",
    "# ============================================================\n",
    "@torch.inference_mode()\n",
    "def generate_answer(instruction: str, user_input: str = \"\") -> str:\n",
    "    user_text = build_user_text(instruction, user_input)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    gen_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=GEN_MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=GEN_TEMPERATURE,\n",
    "        top_p=GEN_TOP_P,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    new_tokens = gen_ids[0, input_ids.shape[1]:]\n",
    "    text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    return text\n",
    "\n",
    "predictions = []\n",
    "for r in tqdm(test_data, desc=\"Generating test predictions\"):\n",
    "    resp = generate_answer(r[\"instruction\"], r[\"input\"])\n",
    "    out = dict(r)\n",
    "    out[\"model_response\"] = resp\n",
    "    predictions.append(out)\n",
    "\n",
    "PRED_PATH = \"predictions_test.json\"\n",
    "with open(PRED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[OK] saved:\", PRED_PATH)\n",
    "print(\"Sample prediction:\\n\", json.dumps(predictions[0], ensure_ascii=False, indent=2)[:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35c18ec4-1e97-436a-b642-c8d79f60f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª QUICK TEST (VAL) â€” 5 samples\n",
      "\n",
      "==========================================================================================\n",
      "[1] idx=1\n",
      "------------------------------------------------------------------------------------------\n",
      "INSTRUCTION:\n",
      "Jawab pertanyaan pengguna berikut dengan jelas, sopan, dan akurat terkait informasi Mirota Kampus.\n",
      "\n",
      "INPUT:\n",
      "Tadi katanya bisa print, tapi ternyata nggak bisa.\n",
      "\n",
      "GROUND TRUTH (dataset output):\n",
      "Mohon maaf atas kebingungannya. Layanan dapat berbeda tergantung cabang dan jam operasional.\n",
      "\n",
      "MODEL RESPONSE:\n",
      "Kami memahami masalah print yang tidak berjalan. Kami sarankan Anda untuk memeriksa ketersediaan stok dan kondisi printer.\n",
      "\n",
      "(Evaluator tersedia tapi gagal jalan): NameError(\"name 'eval_tokenizer' is not defined\")\n",
      "==========================================================================================\n",
      "[2] idx=8\n",
      "------------------------------------------------------------------------------------------\n",
      "INSTRUCTION:\n",
      "Jawab pertanyaan pengguna berikut dengan jelas, sopan, dan akurat terkait informasi Mirota Kampus.\n",
      "\n",
      "INPUT:\n",
      "Mirota ada jual kosmetik juga nggak sih?\n",
      "\n",
      "GROUND TRUTH (dataset output):\n",
      "Ya, tersedia berbagai produk kosmetik dan perawatan diri di beberapa cabang.\n",
      "\n",
      "MODEL RESPONSE:\n",
      "Ya, Mirota Kampus menjual berbagai kebutuhan kosmetik dan hiburan.\n",
      "\n",
      "(Evaluator tersedia tapi gagal jalan): NameError(\"name 'eval_tokenizer' is not defined\")\n",
      "==========================================================================================\n",
      "[3] idx=16\n",
      "------------------------------------------------------------------------------------------\n",
      "INSTRUCTION:\n",
      "Jawab pertanyaan pengguna berikut dengan jelas, sopan, dan akurat terkait informasi Mirota Kampus.\n",
      "\n",
      "INPUT:\n",
      "Kalau anak kos, kebutuhannya bisa kebeli semua nggak di Mirota?\n",
      "\n",
      "GROUND TRUTH (dataset output):\n",
      "Banyak kebutuhan anak kos seperti alat kebersihan dan perlengkapan rumah tangga tersedia di Mirota Kampus.\n",
      "\n",
      "MODEL RESPONSE:\n",
      "Beberapa produk umum tersedia di Mirota Kampus, namun kebijakan dan ketersediaan dapat berbeda.\n",
      "\n",
      "(Evaluator tersedia tapi gagal jalan): NameError(\"name 'eval_tokenizer' is not defined\")\n",
      "==========================================================================================\n",
      "[4] idx=12\n",
      "------------------------------------------------------------------------------------------\n",
      "INSTRUCTION:\n",
      "Jawab pertanyaan pengguna berikut dengan jelas, sopan, dan akurat terkait informasi Mirota Kampus.\n",
      "\n",
      "INPUT:\n",
      "Tadi saya ke Mirota tapi barang yang dicari nggak ada.\n",
      "\n",
      "GROUND TRUTH (dataset output):\n",
      "Mohon maaf atas ketidaknyamanannya. Stok barang bisa berbeda di tiap cabang dan waktu tertentu.\n",
      "\n",
      "MODEL RESPONSE:\n",
      "Kami mengerti masalah itu. Stok bisa berubah sebelum jam tertentu.\n",
      "\n",
      "(Evaluator tersedia tapi gagal jalan): NameError(\"name 'eval_tokenizer' is not defined\")\n",
      "==========================================================================================\n",
      "[5] idx=6\n",
      "------------------------------------------------------------------------------------------\n",
      "INSTRUCTION:\n",
      "Jawab pertanyaan pengguna berikut dengan jelas, sopan, dan akurat terkait informasi Mirota Kampus.\n",
      "\n",
      "INPUT:\n",
      "Ada promo atau diskon?\n",
      "\n",
      "GROUND TRUTH (dataset output):\n",
      "Mirota Kampus sering menyediakan promo musiman dan diskon produk tertentu.\n",
      "\n",
      "MODEL RESPONSE:\n",
      "Promo dan diskon tersedia di beberapa cabang dan waktu tertentu.\n",
      "\n",
      "(Evaluator tersedia tapi gagal jalan): NameError(\"name 'eval_tokenizer' is not defined\")\n",
      "\n",
      "âœ… Selesai QUICK TEST. Lihat apakah MODEL RESPONSE sudah mengikuti pola jawaban dataset kamu.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 10A â€” QUICK TEST GENERATION (cek kesesuaian vs dataset)\n",
    "# ============================================================\n",
    "import random\n",
    "\n",
    "# pilih sumber sampel: \"val\" atau \"test\"\n",
    "QUICK_SOURCE = \"val\"   # ganti ke \"test\" kalau mau\n",
    "N_SAMPLES = 5          # jumlah contoh yang mau dicek\n",
    "SEED_QUICK = 123       # biar konsisten\n",
    "\n",
    "def _short(text: str, max_chars: int = 800) -> str:\n",
    "    text = (text or \"\").strip()\n",
    "    return text if len(text) <= max_chars else text[:max_chars] + \"\\n...[TRUNCATED]...\"\n",
    "\n",
    "# pilih data\n",
    "if QUICK_SOURCE.lower() == \"test\":\n",
    "    pool = test_data\n",
    "else:\n",
    "    pool = val_data\n",
    "\n",
    "if len(pool) == 0:\n",
    "    raise ValueError(\"Pool data kosong. Pastikan val_data/test_data ada.\")\n",
    "\n",
    "random.seed(SEED_QUICK)\n",
    "indices = random.sample(range(len(pool)), k=min(N_SAMPLES, len(pool)))\n",
    "\n",
    "print(f\"\\nðŸ§ª QUICK TEST ({QUICK_SOURCE.upper()}) â€” {len(indices)} samples\\n\")\n",
    "\n",
    "for j, idx in enumerate(indices, start=1):\n",
    "    r = pool[idx]\n",
    "    instr = r[\"instruction\"]\n",
    "    inp   = r[\"input\"]\n",
    "    ref   = r[\"output\"]\n",
    "\n",
    "    pred = generate_answer(instr, inp)\n",
    "\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"[{j}] idx={idx}\")\n",
    "    print(\"-\" * 90)\n",
    "    print(\"INSTRUCTION:\")\n",
    "    print(_short(instr, 600))\n",
    "    print(\"\\nINPUT:\")\n",
    "    print(_short(inp, 600))\n",
    "    print(\"\\nGROUND TRUTH (dataset output):\")\n",
    "    print(_short(ref, 800))\n",
    "    print(\"\\nMODEL RESPONSE:\")\n",
    "    print(_short(pred, 800))\n",
    "\n",
    "    # kalau evaluator scoring sudah ada (CELL 12), tampilkan skor 0â€“5\n",
    "    if \"llm_score_one\" in globals():\n",
    "        try:\n",
    "            s, raw = llm_score_one(instr, inp, ref, pred)\n",
    "            print(\"\\nEVAL SCORE (0â€“5):\", s)\n",
    "            print(\"EVAL RAW:\")\n",
    "            print(_short(raw, 500))\n",
    "        except Exception as e:\n",
    "            print(\"\\n(Evaluator tersedia tapi gagal jalan):\", repr(e))\n",
    "\n",
    "print(\"\\nâœ… Selesai QUICK TEST. Lihat apakah MODEL RESPONSE sudah mengikuti pola jawaban dataset kamu.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e406b47-6ce0-4472-af49-5367511875df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] evaluator loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 11 â€” LOAD EVALUATOR LLM (LLM-based scoring 0â€“5)\n",
    "# Rubrik: Model Evaluation (LLM-based Scoring)\n",
    "# ============================================================\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(EVAL_MODEL, use_fast=True)\n",
    "if eval_tokenizer.pad_token is None:\n",
    "    eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "eval_tokenizer.padding_side = \"right\"\n",
    "\n",
    "eval_model = AutoModelForCausalLM.from_pretrained(EVAL_MODEL, torch_dtype=DTYPE)\n",
    "eval_model.to(DEVICE)\n",
    "eval_model.eval()\n",
    "\n",
    "print(\"[OK] evaluator loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f70afd8-4a21-4f67-a2ef-e68813cf8871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/dltf/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/tljh/user/envs/dltf/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/tljh/user/envs/dltf/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0\n",
      "raw:\n",
      " SCORE: 0\n",
      "REASON: Jawaban yang diberikan tidak sesuai dengan instruksi dan referensi jawaban yang diinginkan. Jelaskanannya lebih lanjut:\n",
      "\n",
      "\"Jawaban yang diberikan tidak mencerminkan bahwa Mirota Kampus menyediakan perlengkapan kos anak. Ini hanya mengatakan bahwa Mirota Kampus memiliki banyak jenis perlengkapan untuk anak kos dan mahasiswa.\"\n",
      "\n",
      "Dengan kata lain, jawaban tersebut tidak menjelaskan secara spesifik tentang apa yang disediakan oleh Mirota Kampus dalam hal perlengk\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 12 â€” SCORING FUNCTION (0â€“5) + HANDLING INVALID\n",
    "# ============================================================\n",
    "EVAL_SYSTEM = (\n",
    "    \"Kamu adalah evaluator jawaban. Beri skor 0 sampai 5.\\n\"\n",
    "    \"0=sepenuhnya salah/halu, 1=sebagian besar salah, 2=kurang tepat, \"\n",
    "    \"3=cukup benar tapi ada kekurangan, 4=benar & jelas, 5=sangat benar, lengkap, dan sesuai instruksi.\\n\"\n",
    "    \"Output WAJIB format:\\n\"\n",
    "    \"SCORE: <angka 0-5>\\n\"\n",
    "    \"REASON: <alasan singkat>\"\n",
    ")\n",
    "\n",
    "def parse_score(text: str) -> int | None:\n",
    "    m = re.search(r\"SCORE\\s*:\\s*([0-5])\", text)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    m2 = re.search(r\"\\b([0-5])\\b\", text)\n",
    "    return int(m2.group(1)) if m2 else None\n",
    "\n",
    "@torch.inference_mode()\n",
    "def llm_score_one(instruction: str, user_input: str, reference: str, model_response: str, max_new_tokens: int = 128):\n",
    "    user_text = build_user_text(instruction, user_input)\n",
    "\n",
    "    prompt = (\n",
    "        \"Nilai jawaban model berdasarkan instruksi dan referensi jawaban benar.\\n\\n\"\n",
    "        f\"INSTRUCTION+INPUT:\\n{user_text}\\n\\n\"\n",
    "        f\"REFERENCE ANSWER:\\n{reference}\\n\\n\"\n",
    "        f\"MODEL ANSWER:\\n{model_response}\\n\\n\"\n",
    "        \"Beri skor 0-5.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": EVAL_SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    input_ids = eval_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    gen_ids = eval_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        eos_token_id=eval_tokenizer.eos_token_id,\n",
    "        pad_token_id=eval_tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    new_tokens = gen_ids[0, input_ids.shape[1]:]\n",
    "    text = eval_tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    score = parse_score(text)\n",
    "    if score is None:\n",
    "        # handling invalid score â†’ default 0 (rubrik minta handling invalid)\n",
    "        score = 0\n",
    "\n",
    "    return int(score), text\n",
    "\n",
    "# quick test\n",
    "sample = predictions[0]\n",
    "s, raw = llm_score_one(sample[\"instruction\"], sample[\"input\"], sample[\"output\"], sample[\"model_response\"])\n",
    "print(\"score:\", s)\n",
    "print(\"raw:\\n\", raw[:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b430167a-a71a-40df-8b87-a6feff3cea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring predictions (0-5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:28<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved: eval_scores.json\n",
      "STATS: {\n",
      "  \"n\": 17,\n",
      "  \"mean\": 0.6470588235294118,\n",
      "  \"median\": 0.0,\n",
      "  \"min\": 0,\n",
      "  \"max\": 2,\n",
      "  \"distribution\": {\n",
      "    \"0\": 11,\n",
      "    \"1\": 1,\n",
      "    \"2\": 5,\n",
      "    \"3\": 0,\n",
      "    \"4\": 0,\n",
      "    \"5\": 0\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 13 â€” SCORE ALL + SAVE + STATS\n",
    "# ============================================================\n",
    "scored = []\n",
    "for r in tqdm(predictions, desc=\"Scoring predictions (0-5)\"):\n",
    "    s, raw = llm_score_one(r[\"instruction\"], r[\"input\"], r[\"output\"], r[\"model_response\"])\n",
    "    r2 = dict(r)\n",
    "    r2[\"score_0_5\"] = int(s)\n",
    "    r2[\"score_raw\"] = raw\n",
    "    scored.append(r2)\n",
    "\n",
    "EVAL_PATH = \"eval_scores.json\"\n",
    "with open(EVAL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scored, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "arr = np.array([x[\"score_0_5\"] for x in scored], dtype=np.int32)\n",
    "stats = {\n",
    "    \"n\": int(arr.size),\n",
    "    \"mean\": float(arr.mean()) if arr.size else 0.0,\n",
    "    \"median\": float(np.median(arr)) if arr.size else 0.0,\n",
    "    \"min\": int(arr.min()) if arr.size else 0,\n",
    "    \"max\": int(arr.max()) if arr.size else 0,\n",
    "    \"distribution\": {str(i): int((arr == i).sum()) for i in range(6)},\n",
    "}\n",
    "\n",
    "print(\"[OK] saved:\", EVAL_PATH)\n",
    "print(\"STATS:\", json.dumps(stats, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b15bc07-a328-4543-b0e6-1abfd3bf3afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… OUTPUT untuk rubrik:\n",
      "- splits_json/train.json, val.json, test.json\n",
      "- output_lora/ (adapter + tokenizer)\n",
      "- predictions_test.json (dengan model_response)\n",
      "- eval_scores.json (dengan score_0_5 + score_raw) dan statistik di console\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 14 â€” DONE (CHECKLIST OUTPUT)\n",
    "# ============================================================\n",
    "print(\"\\nâœ… OUTPUT untuk rubrik:\")\n",
    "print(\"- splits_json/train.json, val.json, test.json\")\n",
    "print(\"- output_lora/ (adapter + tokenizer)\")\n",
    "print(\"- predictions_test.json (dengan model_response)\")\n",
    "print(\"- eval_scores.json (dengan score_0_5 + score_raw) dan statistik di console\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python for Deep Learning using Tensorflow",
   "language": "python",
   "name": "dltf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
